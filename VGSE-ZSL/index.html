<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning">
    <meta name="author" content="Wenjia Xu,
                                Yongqin Xian,
				 Jiuniu Wang,
                                Zeynep Akata,
				 Bernt Schiele">

    <title>VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning</h2>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        <a > Wenjia Xu</a>,
        <a >Yongqin Xian</a>,
	    <a >Jiuniu Wang</a>,
        <a >Zeynep Akata</a>,
	    <a >Bernt Schiele</a>
        
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a href="https://arxiv.org/abs/2203.10444" class="btn btn-primary">Paper</a>
        
<!-- 	<a href="https://arxiv.org/abs/2007.06877" class="btn btn-primary">Paper</a> -->

        <a href="https://github.com/wenjiaXu/VGSE" class="btn btn-primary">Code</a>
        
        <a href="https://www.eml-unitue.de/publication/Visually-Grounded-Semantic-Embeddings-for-Zero-Shot-Learning" class="btn btn-primary">English Blog</a>

        <a href="https://arxiv.org/abs/2203.10444" class="btn btn-primary">中文介绍</a>
    </div>
</div>

<div class="container">
	<h2>Introduction</h2>
    <div class="section">

        <hr>
        <p>
            Semantic embeddings aggregated for every class live in a vector space that associates different classes, even when visual examples of these classes are not available. Therefore, they facilitate the knowledge transfer in zero-shot learning (ZSL) and are used as side-information in other computer vision tasks like fashion trend forecast, face recognition and manipulation. Human annotated attributes are widely used semantic embeddings. However, obtaining attributes is often a labor-intensive process. Previous work tackle this problem by using word embeddings for class names, or semantic embeddings from online encyclopedia articles. However, some of these relations may not be visually detectable by machines, resulting in a poor performance in zero-shot learning.  </p>

            <div  align="center"> 
                <img src="./images/teaser_figure.png" width="90%" alt="teaser figure" align=center />
        
            </div>
            <p>
                To this end, we propose the Visually-Grounded Semantic Embedding (VGSE) Network to discover semantic embeddings with minimal human supervision (we only use category labels for seen class images). To fully unearth the visual properties shared across different categories, our model discovers semantic embeddings by assigning image patches into various clusters according to their visual similarity. Besides, we further impose class discrimination and semantic relatedness of the semantic embeddings, to benefit their ability in transferring knowledge between classes in ZSL. 
            </p>
    </div>

    
    

    <div class="section">
        <h2>Qualitative Results</h2>
        <hr>
        <p>
            We show the 2D visualization of image patches in the AWA2, where $10,000$ image patches are presented by projecting their embeddings $a_{nt}$ onto two dimensions with t-SNE. To picture the distribution of the semantic embedding space, we sample several visual clusters~(dots marked in the same color) and the image patches from the cluster center of both seen and unseen categories.
        </p>
        <div  align="center"> 
            <img src="./images/t-SNE-2.png" width="60%" alt="teaser figure" align=center />
        </div>
        <p>
            We observe that samples in the same cluster tend to gather together, indicating that the embeddings provide discriminative information. Besides, images patches in one cluster do convey consistent visual properties, though coming from disjoint categories. For instance, the white fur appears on rabbit, polar bear, and fox are clustered into one group. We further observe that nearly all clusters consist images from more than one categories. It indicates that the clusters we learned contain semantic properties shared across seen classes, and can be transferred to unseen classes. Another interesting observation is that our VGSE clusters discover visual properties that my be neglected by human-annotated attributes, e.g., the cage appear for hamsters and rat. 
        </p>
	
	

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="http://wenjia.ruantang.top/">Wenjia Xu</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
